**ICU-tokenizer** is a python package used to perform universal language
normalization and tokenization using the International Components for
Unicode.

- [Install](#install)
- [Usage (Python)](#usage-python)
  - [Sentence splitter](#sentence-splitter)
  - [Normalizer](#normalizer)
  - [Tokenizer](#tokenizer)

## Install

See [./INSTALL.md](./INSTALL.md)

## Usage (Python)

### Sentence splitter

```py
# To split a paragraph into multiple sentences
>>> from icu_tokenizer import SentSplitter
>>> splitter = SentSplitter('zh')

>>> paragraph = """
ÁæéÂõΩÊúÄÈ´òÊ≥ïÈô¢ÔºàËã±ËØ≠ÔºöSupreme Court of the United StatesÔºâÔºå‰∏ÄËà¨ÊòØÊåáÁæéÂõΩËÅîÈÇ¶ÊúÄÈ´òÊ≥ïÈô¢ÔºåÊòØÁæéÂõΩÊúÄÈ´òÁ∫ßÂà´ÁöÑËÅîÈÇ¶Ê≥ïÈô¢Ôºå‰∏∫ÁæéÂõΩ‰∏âÊùÉÁªßÊÄªÁªü„ÄÅÂõΩ‰ºöÂêéÊúÄ‰∏∫ÈáçË¶ÅÁöÑ‰∏ÄÁéØ„ÄÇÊ†πÊçÆ1789Âπ¥„ÄäÁæéÂõΩÂÆ™Ê≥ïÁ¨¨‰∏âÊù°„ÄãÁöÑËßÑÂÆöÔºåÊúÄÈ´òÊ≥ïÈô¢ÂØπÊâÄÊúâËÅîÈÇ¶Ê≥ïÈô¢„ÄÅÂ∑ûÊ≥ïÈô¢ÂíåÊ∂âÂèäËÅîÈÇ¶Ê≥ïÂæãÈóÆÈ¢òÁöÑËØâËÆºÊ°à‰ª∂ÂÖ∑ÊúâÊúÄÁªàÔºàÂπ∂‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÊúâÊñüÈÖåÂÜ≥ÂÆöÊùÉÁöÑÔºâ‰∏äËØâÁÆ°ËæñÊùÉÔºå‰ª•ÂèäÂØπÂ∞èËåÉÂõ¥Ê°à‰ª∂ÁöÑÂÖ∑ÊúâÂàùÂÆ°ÁÆ°ËæñÊùÉ„ÄÇÂú®ÁæéÂõΩÁöÑÊ≥ïÂæãÂà∂Â∫¶‰∏≠ÔºåÊúÄÈ´òÊ≥ïÈô¢ÈÄöÂ∏∏ÊòØÂåÖÊã¨„ÄäÁæéÂõΩÂÆ™Ê≥ï„ÄãÂú®ÂÜÖÁöÑËÅîÈÇ¶Ê≥ïÂæãÁöÑÊúÄÁªàËß£ÈáäËÄÖÔºå‰ΩÜ‰ªÖÂú®ÂÖ∑ÊúâÁÆ°ËæñÊùÉÁöÑÊ°à‰ª∂ËåÉÂõ¥ÂÜÖ„ÄÇÊ≥ïÈô¢‰∏ç‰∫´ÊúâÂà§ÂÆöÊîøÊ≤ªÈóÆÈ¢òÁöÑÊùÉÂäõÔºõÊîøÊ≤ªÈóÆÈ¢òÁöÑÊâßÊ≥ïÊú∫ÂÖ≥ÊòØË°åÊîøÊú∫ÂÖ≥ÔºåËÄå‰∏çÊòØÊîøÂ∫úÁöÑÂè∏Ê≥ïÈÉ®Èó®„ÄÇ
"""
>>> splitter.split(paragraph)
[
    'ÁæéÂõΩÊúÄÈ´òÊ≥ïÈô¢ÔºàËã±ËØ≠ÔºöSupreme Court of the United StatesÔºâÔºå‰∏ÄËà¨ÊòØÊåáÁæéÂõΩËÅîÈÇ¶ÊúÄÈ´òÊ≥ïÈô¢ÔºåÊòØÁæéÂõΩÊúÄÈ´òÁ∫ßÂà´ÁöÑËÅîÈÇ¶Ê≥ïÈô¢Ôºå‰∏∫ÁæéÂõΩ‰∏âÊùÉÁªßÊÄªÁªü„ÄÅÂõΩ‰ºöÂêéÊúÄ‰∏∫ÈáçË¶ÅÁöÑ‰∏ÄÁéØ„ÄÇ',
    'Ê†πÊçÆ1789Âπ¥„ÄäÁæéÂõΩÂÆ™Ê≥ïÁ¨¨‰∏âÊù°„ÄãÁöÑËßÑÂÆöÔºåÊúÄÈ´òÊ≥ïÈô¢ÂØπÊâÄÊúâËÅîÈÇ¶Ê≥ïÈô¢„ÄÅÂ∑ûÊ≥ïÈô¢ÂíåÊ∂âÂèäËÅîÈÇ¶Ê≥ïÂæãÈóÆÈ¢òÁöÑËØâËÆºÊ°à‰ª∂ÂÖ∑ÊúâÊúÄÁªàÔºàÂπ∂‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÊúâÊñüÈÖåÂÜ≥ÂÆöÊùÉÁöÑÔºâ‰∏äËØâÁÆ°ËæñÊùÉÔºå‰ª•ÂèäÂØπÂ∞èËåÉÂõ¥Ê°à‰ª∂ÁöÑÂÖ∑ÊúâÂàùÂÆ°ÁÆ°ËæñÊùÉ„ÄÇ',
    'Âú®ÁæéÂõΩÁöÑÊ≥ïÂæãÂà∂Â∫¶‰∏≠ÔºåÊúÄÈ´òÊ≥ïÈô¢ÈÄöÂ∏∏ÊòØÂåÖÊã¨„ÄäÁæéÂõΩÂÆ™Ê≥ï„ÄãÂú®ÂÜÖÁöÑËÅîÈÇ¶Ê≥ïÂæãÁöÑÊúÄÁªàËß£ÈáäËÄÖÔºå‰ΩÜ‰ªÖÂú®ÂÖ∑ÊúâÁÆ°ËæñÊùÉÁöÑÊ°à‰ª∂ËåÉÂõ¥ÂÜÖ„ÄÇ',
    'Ê≥ïÈô¢‰∏ç‰∫´ÊúâÂà§ÂÆöÊîøÊ≤ªÈóÆÈ¢òÁöÑÊùÉÂäõÔºõÊîøÊ≤ªÈóÆÈ¢òÁöÑÊâßÊ≥ïÊú∫ÂÖ≥ÊòØË°åÊîøÊú∫ÂÖ≥ÔºåËÄå‰∏çÊòØÊîøÂ∫úÁöÑÂè∏Ê≥ïÈÉ®Èó®„ÄÇ'
]
```

### Normalizer

```py
# To normalize text
>>> from icu_tokenizer import Normalizer
>>> normalizer = Normalizer(lang='en', norm_puncts=True)

>>> text = "ùëªùíâùíÜ ùíëùíìùíêùíÖùíñùíÑùíïùíî ùíöùíêùíñ ùíêùíìùíÖùíÜùíìùíÜùíÖ ùíòùíäùíçùíç ùíÉùíÜ ùíîùíâùíäùíëùíëùíÜùíÖ ùíÖùíäùíìùíÜùíÑùíïùíçùíö ùíáùíìùíêùíé ùë≤ùíêùíìùíÜùíÇ."
>>> normalizer.normalize(text)
"The products you ordered will be shipped directly from Korea."

>>> text = "„Äê„ÄëÔºàÔºâ"
>>> normalizer.normalize(text)
"[]()"
```

### Tokenizer

```py
>>> from icu_tokenizer import Tokenizer
>>> tokenizer = Tokenizer(lang='th')

>>> text = "‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô ‡πÅ‡∏•‡∏∞‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥"
>>> tokenizer.tokenize(text)
['‡∏†‡∏≤‡∏©‡∏≤', '‡πÑ‡∏ó‡∏¢', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏ó‡∏µ‡πà', '‡∏°‡∏µ', '‡∏£‡∏∞‡∏î‡∏±‡∏ö', '‡πÄ‡∏™‡∏µ‡∏¢‡∏á', '‡∏Ç‡∏≠‡∏á', '‡∏Ñ‡∏≥', '‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô', '‡∏´‡∏£‡∏∑‡∏≠', '‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå', '‡πÄ‡∏ä‡πà‡∏ô', '‡πÄ‡∏î‡∏µ‡∏¢‡∏ß', '‡∏Å‡∏±‡∏ö', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏à‡∏µ‡∏ô', '‡πÅ‡∏•‡∏∞', '‡∏≠‡∏≠‡∏Å', '‡πÄ‡∏™‡∏µ‡∏¢‡∏á', '‡πÅ‡∏¢‡∏Å', '‡∏Ñ‡∏≥', '‡∏ï‡πà‡∏≠', '‡∏Ñ‡∏≥']
```
